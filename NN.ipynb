{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self,iL_s, oL_s, hL_n, hL_s, iActFunc, hActFunc,reg, learningRate, toStop, stopSize):\n",
    "        #layer sizes:\n",
    "        self.inputLayerSize = iL_s;\n",
    "        self.outputLayerSize = oL_s;\n",
    "        self.hiddenLayers = hL_n;\n",
    "        self.hiddenLayerSize = hL_s;\n",
    "        #wheight intialization, using Xavier initailization:\n",
    "        self.W = []\n",
    "        self.W.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0])/np.sqrt(self.inputLayerSize/2.0));\n",
    "        for i in range(self.hiddenLayers-1):\n",
    "            self.W.append(np.random.randn(self.hiddenLayerSize[i], self.hiddenLayerSize[i+1])/np.sqrt(self.hiddenLayerSize[i]/2.0));\n",
    "        self.W.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize)/np.sqrt(self.hiddenLayerSize[-1]/2.0));  \n",
    "       \n",
    "        #bais intialization\n",
    "        self.b = [];\n",
    "        self.b.append(np.random.randn(1, self.hiddenLayerSize[0])/np.sqrt(self.inputLayerSize/2.0));\n",
    "        for i in range(self.hiddenLayers-1):\n",
    "            self.b.append(np.random.randn(1, self.hiddenLayerSize[i+1])/np.sqrt(self.hiddenLayerSize[i]/2.0));\n",
    "        self.b.append(np.random.randn(1, self.outputLayerSize)/np.sqrt(self.hiddenLayerSize[-1]/2.0));  \n",
    "        \n",
    "        #choosing activation function:\n",
    "        self.inputActivationFunction = iActFunc;\n",
    "        self.hiddenActivationFunction = hActFunc;\n",
    "        #output activation is set to be softmax for the ease of using NLL\n",
    "        #adding hyperparameters\n",
    "        self.reg = reg;\n",
    "        self.alpha = learningRate;\n",
    "        #loss history intialization:\n",
    "        self.toStop = toStop;\n",
    "        self.stopSize = stopSize;\n",
    "        self.stopNow = False;\n",
    "        if(self.toStop):\n",
    "            self.lossHist = np.zeros(self.stopSize);\n",
    "            self.lossHist[self.lossHist == 0] = np.inf; \n",
    "        ##### this is used to store the weight set that produced the best result\n",
    "        self.BW = self.W;\n",
    "        self.Bb = self.b;\n",
    "    ###########################\n",
    "    def relu(self, x):    #relu activation function\n",
    "        x[x<=0] = 0;\n",
    "        return x;\n",
    "    \n",
    "    def sigmoid(self,x):  #sigmoid activation function\n",
    "        return 1/(1+np.exp(-x));\n",
    "    \n",
    "    def tanh(self, x):    # tanh activation function\n",
    "        return np.tanh(x);\n",
    "    \n",
    "    def elu(self, x):     #elu activation function\n",
    "        x[x<=0] = self.alpha * (np.exp(x[x<=0])-1);\n",
    "        return x;\n",
    " \n",
    "    def softmax(self,x):  #softmax implementation\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True));   # here we subtract the maximum value to avoid the overflow\n",
    "        \n",
    "        probs = exp_x / (np.sum(exp_x, axis=1, keepdims=True)); #softmax process\n",
    "        \n",
    "        return probs;\n",
    "     \n",
    "        \n",
    "   ####################################     \n",
    "    def forward(self, x):\n",
    "        self.a = [];  #array to store output of activation layers\n",
    "        self.t = [];  #array to store output of linear parts\n",
    "        self.t.append(np.dot(x, np.array(self.W[0])) + np.array(self.b[0]))  # calculate the linear part of the input layer\n",
    "        \n",
    "        \n",
    "        #based on the activation type go through the activation\n",
    "        if(self.inputActivationFunction =='relu'):\n",
    "            self.a.append(self.relu(np.array(self.t[0])))\n",
    "        elif(self.inputActivationFunction =='sigmoid'):\n",
    "            self.a.append(self.sigmoid(np.array(self.t[0])))\n",
    "        elif(self.inputActivationFunction =='elu'):\n",
    "            self.a.append(self.elu(np.array(self.t[0])))\n",
    "        elif(self.inputActivationFunction =='tanh'):\n",
    "            self.a.append(self.tanh(np.array(self.t[0])))\n",
    "        else: #if no activation is used on the input\n",
    "            self.a.append(self.t[0]);\n",
    "        \n",
    "        #go through the hidden layers repeating the same process\n",
    "        for i in range (1, self.hiddenLayers+1):\n",
    "            self.t.append(np.dot(np.array(self.a[i-1]), np.array(self.W[i]))+np.array(self.b[i]))\n",
    "            #use the activation functions for hidden layers unless you reach the output layer\n",
    "            if (i != self.hiddenLayers):\n",
    "                if(self.hiddenActivationFunction =='relu'):\n",
    "                    self.a.append(self.relu(np.array(self.t[i])));\n",
    "                elif(self.hiddenActivationFunction =='sigmoid'):\n",
    "                    self.a.append(self.sigmoid(np.array(self.t[i])));\n",
    "                elif(self.hiddenActivationFunction =='elu'):\n",
    "                    self.a.append(self.elu(np.array(self.t[i])));\n",
    "                elif(self.hiddenActivationFunction =='tanh'):\n",
    "                    self.a.append(self.tanh(np.array(self.t[0])))\n",
    "                else: \n",
    "                    self.a.append(self.t[0]);\n",
    "        #apply softmax on the output layer\n",
    "        self.yHat = self.softmax(np.array(self.t[-1]));\n",
    "        \n",
    "       \n",
    "  \n",
    "    ###################################3     \n",
    "    def loss(self, y, Test):  \n",
    "        #loss calculation\n",
    "        n = self.yHat.shape[0]\n",
    "        data_loss = -np.sum(np.log(self.yHat[np.arange(n), y]+1e-9)) / n #negative log likelihood\n",
    "        #reguralization loss\n",
    "        reg_loss = 0\n",
    "        for i in range(len(self.W)):\n",
    "            reg_loss += 0.5*self.reg*np.sum(self.W[i]*self.W[i]);  \n",
    "        loss = data_loss + reg_loss  \n",
    "        \n",
    "        if (Test):   #if you're testing; don't account for regularization\n",
    "            return data_loss;\n",
    "        else:      #else account for it\n",
    "            return loss;\n",
    "\n",
    "    def sigmoidDerivative(self,x): #derivative of sigmoid activation\n",
    "        return np.exp(-x)/((1+np.exp(-x))**2)\n",
    "    \n",
    "    def reluDerivative(self,x):   #derivative of relu activation\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "    def eluDerivative(self, x):  #derivative of elu activation\n",
    "        x[x<0] = self.alpha * (np.exp(x[x<0]));\n",
    "        x[x>0] = 1;\n",
    "        return x;\n",
    " \n",
    "    def tanhDerivative(self,x):  #derivative of tanh activation\n",
    "        return 1.0 - np.tanh(x)**2\n",
    "    \n",
    "    ##################################\n",
    "    def backward(self, x, y):\n",
    "        n = y.shape[0]\n",
    "        #calculate the derivative for softmax and NLL;\n",
    "        dscores = self.yHat;\n",
    "        dscores[range(n),y] -= 1\n",
    "        dscores /= n\n",
    "\n",
    "        dw = []\n",
    "        db = []\n",
    "        dhidden = dscores\n",
    "        \n",
    "        \n",
    "        #back probagate through the output layer calculating the first gradient of weights\n",
    "        dw.append(np.dot(np.array(self.a[-1]).T, dscores))\n",
    "        db.append(np.sum(dscores, axis=0, keepdims=True));             \n",
    "        #back probagate through the neural network\n",
    "        for i in range (1, len(self.W)):\n",
    "                #calculate the gradient\n",
    "                dhidden = np.dot(dhidden, np.array(self.W[0-i]).T)\n",
    "                    \n",
    "                if (i == len(self.W)-1):#if we reached input layer\n",
    "                    #go through input layer activation\n",
    "                    if(self.inputActivationFunction =='relu'):\n",
    "                        dhidden = np.multiply(dhidden , self.reluDerivative(self.a[-i]));\n",
    "                    elif(self.inputActivationFunction =='sigmoid'):\n",
    "                        dhidden = np.multiply(dhidden , self.sigmoidDerivative(self.a[-i]));\n",
    "                    elif(self.inputActivationFunction =='elu'):\n",
    "                        dhidden = np.multiply(dhidden , self.eluDerivative(self.a[-i]));\n",
    "                    elif(self.inputActivationFunction =='tanh'):\n",
    "                        dhidden = np.multiply(dhidden , self.tanhDerivative(self.a[-i]));\n",
    "                    #calculate the gradient of the weight and the bais\n",
    "                    dw.append(np.dot(x.T, dhidden))\n",
    "                    db.append(np.sum(dhidden, axis=0, keepdims=True))\n",
    "                else: #if not\n",
    "                    #go through hidden layers activation\n",
    "                    if(self.hiddenActivationFunction =='relu'):\n",
    "                        dhidden = np.multiply(dhidden , self.reluDerivative(self.a[-i]));\n",
    "                    elif(self.hiddenActivationFunction =='sigmoid'):\n",
    "                        dhidden = np.multiply(dhidden , self.sigmoidDerivative(self.a[-i]));\n",
    "                    elif(self.hiddenActivationFunction =='elu'):\n",
    "                        dhidden = np.multiply(dhidden , self.eluDerivative(self.a[-i]));\n",
    "                    elif(self.hiddenActivationFunction =='tanh'):\n",
    "                        dhidden = np.multiply(dhidden , self.tanhDerivative(self.a[-i]));\n",
    "                    #calculate the gradient of the weight and the bais\n",
    "                    dw.append(np.dot(np.array(self.a[-1-i]).T, dhidden))\n",
    "                    db.append(np.sum(dhidden, axis=0, keepdims=True))\n",
    "                    \n",
    "        #add the reguralization gradient   \n",
    "        for i in range (0, len(dw)):\n",
    "            dw[i] += self.reg * self.W[-1-i]\n",
    "       \n",
    "        return dw, db; \n",
    "\n",
    "    \n",
    "    \n",
    "    ############################################\n",
    "    def trainbatch(self, x,y): #training a single batch\n",
    "        #go forward\n",
    "        self.forward(x);\n",
    "        #calculate the loss\n",
    "        loss = self.loss(y,False);\n",
    "        #go backward\n",
    "        dw, db = self.backward(x, y);\n",
    "        #update the weights using SGD\n",
    "        for j in range(len(self.W)):\n",
    "            self.W[j] = np.array(self.W[j]) - self.alpha*np.array(dw[0-j-1]);\n",
    "            self.b[j] = np.array(self.b[j]) - self.alpha*np.array(db[0-j-1]);           \n",
    "        \n",
    "        \n",
    "         #check the predicted class for accuracy calculation\n",
    "        predicted_class = np.argmax(self.yHat, axis=1)\n",
    "        cnt = 0\n",
    "        for i in range (0, len(y)):\n",
    "            if (y[i] == predicted_class[i]):#if correctly classified\n",
    "                cnt += 1 #add 1\n",
    "        return (cnt/float(len(x))),loss #return the accuracy and loss for this batch\n",
    "        \n",
    "    #####################################################\n",
    "    def validate(self, x, y): #validate during the training\n",
    "        #go forward\n",
    "        self.forward(x);\n",
    "        #calculate the data loss\n",
    "        loss = self.loss(y, True);\n",
    "         #check the predicted class for accuracy calculation\n",
    "        predicted_class = np.argmax(self.yHat, axis=1)\n",
    "        cnt = 0\n",
    "        for i in range (0, len(y)):\n",
    "            if (y[i] == predicted_class[i]):#if correctly classified\n",
    "                cnt += 1;#add 1\n",
    "        return  (cnt/float(len(x))),loss #return the accuracy and loss for this set\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################################\n",
    "    \n",
    "    def stopE(self,loss): #force stop logic\n",
    "        #initialize a probablity array of zeros\n",
    "        probs = np.zeros(self.stopSize)\n",
    "        #loop over the cached loss history and for each loss data that is less than the current loss make the probability of stop 1\n",
    "        probs[self.lossHist < loss] = 1;\n",
    "        #save the current loss into the array\n",
    "        self.lossHist[:-1] = self.lossHist[1:];\n",
    "        self.lossHist[-1] = loss;\n",
    "        \n",
    "        if(np.mean(probs) > 0.8):#if more 80% of the previous losses are less than the current loss\n",
    "            if(self.stopNow): # if you already faced a similar situation in the previous iteration\n",
    "                return True; #stop the training\n",
    "            else: #if not\n",
    "                self.stopNow = True; #record the incident \n",
    "                return False;      #don't stop the training now\n",
    "        else:\n",
    "            self.stopNow = False; #disregard previous alarms\n",
    "            return False; #don't stop the training\n",
    "            \n",
    "        \n",
    "    ###########################################################\n",
    "    def train(self, x, y, epoch = 1000, batchSize = 128, validation = 1000):\n",
    "        #decide how many batches do you have based on the batchsize and input size \n",
    "        y = np.array(y);\n",
    "        batchN = (len(x)-validation)/batchSize; #leave out a part for validation\n",
    "        bestAcc = 0; # to record the best validation Acc. achieved \n",
    "        #prepare for the epoch Vs loss graph\n",
    "        plt.ion();\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        #loop over the epochs\n",
    "        for i in range (epoch):\n",
    "            acc = 0  #reset accuracy\n",
    "            loss = 0 #reset loss\n",
    "            \n",
    "            #loop over the batches\n",
    "            for k in range (batchN):\n",
    "                #train\n",
    "                tmp1, tmp2 = self.trainbatch(x[k*batchSize:(k+1)*batchSize],y[k*batchSize:(k+1)*batchSize]); \n",
    "                acc += tmp1 #accumulate accuracy\n",
    "                loss += tmp2 #accumulate loss\n",
    "                \n",
    "            #record loss and accuracy of the validation set\n",
    "            vAcc, vLoss = self.validate(x[len(x)-validation:],y[len(y)-validation:]);\n",
    "            \n",
    "            #add to the plot\n",
    "            plt.plot(i, loss/float(batchN), 'k', marker = '^')\n",
    "            plt.plot(i, vLoss, 'b', marker = 'o')\n",
    "            #printing givens on the current epoch\n",
    "            print(\"epoch: \", i, \"Tr. loss: \", (loss/float(batchN)), \"Tr. Acc: \", (acc/float(batchN))*100, \"%\", \"Val. Loss: \", vLoss,\"Val. Acc: \", vAcc*100,\"%\")\n",
    "            \n",
    "            #check if this configuration has achieved a higher accuracy than before\n",
    "            if(vAcc*100 >= bestAcc):\n",
    "                #record the configuration for future use\n",
    "                bestAcc = vAcc*100;\n",
    "                self.BW = self.W;\n",
    "                self.Bb = self.b\n",
    "                \n",
    "            #check if we should stop the training now\n",
    "            if((self.toStop == True) and (self.stopE(vLoss) == True) and (self.stopNow == True)):\n",
    "                break;\n",
    "            \n",
    "    ##############################################\n",
    "    def test(self, x, y):\n",
    "        #make the weights and baises as the best recorded by the training\n",
    "        self.W = self.BW;\n",
    "        self.b = self.Bb;\n",
    "        #forward\n",
    "        self.forward(x);\n",
    "        #calculate data loss\n",
    "        loss = self.loss(y, True);\n",
    "        \n",
    "        #calculate the accuracy\n",
    "        predicted_class = np.argmax(self.yHat, axis=1)\n",
    "        cnt = 0\n",
    "        for f in range (0, len(y)):\n",
    "            if (y[f] == predicted_class[f]):\n",
    "                cnt += 1;\n",
    "        print(\"test data - loss: \", loss, \"Acc: \", (cnt/float(len(x)))*100, \"%\");\n",
    "        return loss, ((cnt/float(len(x)))*100)\n",
    "    \n",
    "    ##########################################\n",
    "    def testPerClass(self,x,y, classes):\n",
    "       #make the weights and baises as the best recorded by the training\n",
    "        self.W = self.BW;\n",
    "        self.b = self.Bb;\n",
    "        #go forward\n",
    "        self.forward(x);\n",
    "        #calculate loss\n",
    "        loss = self.loss(y, True);\n",
    "        \n",
    "        #calculate the accuracy for each class \n",
    "        predicted_class = np.argmax(self.yHat, axis=1)\n",
    "        corr = np.zeros(classes);\n",
    "        for f in range (0, len(y)):\n",
    "            if (y[f] == predicted_class[f]):\n",
    "                corr[predicted_class[f]] += 1;\n",
    "        #printing the class accuracy\n",
    "        for i in range (len(corr)):\n",
    "            print (\"class: \", i, \" acc: \", (corr[i]/(len(y)/classes))*100);\n",
    "        \n",
    "    #############################################\n",
    "    def printBW(self): #this is used to return the best weight and bais, to use it to continue the training where it stopped\n",
    "        return self.BW, self.Bb;\n",
    "    \n",
    "    ############################################\n",
    "    def initW(self, NW, Nb): #re-initializing the weights to continue the training when it stops\n",
    "        self.W = NW;\n",
    "        self.b = Nb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the training data file\n",
    "file = \"train\";\n",
    "dict = unpickle(file); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracting the data and labels from the dictionary\n",
    "Darr = dict['data'];\n",
    "Larr = dict['coarse_labels'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reding the test data\n",
    "file = \"test\";\n",
    "dict = unpickle(file); #reading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Tarr = dict['data']; #reading input data\n",
    "TLarr = dict['coarse_labels']; #reading input labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "50000\n",
      "('epoch: ', 0, 'Tr. loss: ', 19.79082596170079, 'Tr. Acc: ', 12.293750000000001, '%', 'Val. Loss: ', 2.7847243846777716, 'Val. Acc: ', 15.35, '%')\n",
      "('epoch: ', 1, 'Tr. loss: ', 19.356888752645283, 'Tr. Acc: ', 18.795833333333334, '%', 'Val. Loss: ', 2.658544094332248, 'Val. Acc: ', 18.6, '%')\n",
      "('epoch: ', 2, 'Tr. loss: ', 19.125207998531145, 'Tr. Acc: ', 21.847916666666666, '%', 'Val. Loss: ', 2.5874760530096563, 'Val. Acc: ', 21.15, '%')\n",
      "('epoch: ', 3, 'Tr. loss: ', 18.93235690770558, 'Tr. Acc: ', 23.939583333333335, '%', 'Val. Loss: ', 2.5402960746410344, 'Val. Acc: ', 22.35, '%')\n",
      "('epoch: ', 4, 'Tr. loss: ', 18.758700347701595, 'Tr. Acc: ', 25.414583333333336, '%', 'Val. Loss: ', 2.5058147861244553, 'Val. Acc: ', 23.5, '%')\n",
      "('epoch: ', 5, 'Tr. loss: ', 18.596666937967058, 'Tr. Acc: ', 26.739583333333332, '%', 'Val. Loss: ', 2.479795694450898, 'Val. Acc: ', 24.2, '%')\n",
      "('epoch: ', 6, 'Tr. loss: ', 18.442472081822167, 'Tr. Acc: ', 27.835416666666667, '%', 'Val. Loss: ', 2.4582995280229563, 'Val. Acc: ', 24.7, '%')\n",
      "('epoch: ', 7, 'Tr. loss: ', 18.294159421174562, 'Tr. Acc: ', 28.818749999999998, '%', 'Val. Loss: ', 2.4397700229952397, 'Val. Acc: ', 25.5, '%')\n",
      "('epoch: ', 8, 'Tr. loss: ', 18.150379003479824, 'Tr. Acc: ', 29.533333333333335, '%', 'Val. Loss: ', 2.424194794855251, 'Val. Acc: ', 25.7, '%')\n",
      "('epoch: ', 9, 'Tr. loss: ', 18.01038325073345, 'Tr. Acc: ', 30.25625, '%', 'Val. Loss: ', 2.410402190825117, 'Val. Acc: ', 25.75, '%')\n",
      "('epoch: ', 10, 'Tr. loss: ', 17.873471549851637, 'Tr. Acc: ', 30.927083333333332, '%', 'Val. Loss: ', 2.3979523825947244, 'Val. Acc: ', 26.0, '%')\n",
      "('epoch: ', 11, 'Tr. loss: ', 17.739308318567716, 'Tr. Acc: ', 31.510416666666668, '%', 'Val. Loss: ', 2.386582746460964, 'Val. Acc: ', 26.55, '%')\n",
      "('epoch: ', 12, 'Tr. loss: ', 17.60770203346213, 'Tr. Acc: ', 32.08541666666667, '%', 'Val. Loss: ', 2.3760229248631735, 'Val. Acc: ', 27.05, '%')\n",
      "('epoch: ', 13, 'Tr. loss: ', 17.478254654550565, 'Tr. Acc: ', 32.64791666666667, '%', 'Val. Loss: ', 2.366663711892233, 'Val. Acc: ', 27.900000000000002, '%')\n",
      "('epoch: ', 14, 'Tr. loss: ', 17.35085372188757, 'Tr. Acc: ', 33.06458333333334, '%', 'Val. Loss: ', 2.3577773929460975, 'Val. Acc: ', 28.1, '%')\n",
      "('epoch: ', 15, 'Tr. loss: ', 17.22533091866638, 'Tr. Acc: ', 33.479166666666664, '%', 'Val. Loss: ', 2.3492766558612646, 'Val. Acc: ', 28.349999999999998, '%')\n",
      "('epoch: ', 16, 'Tr. loss: ', 17.10156165429995, 'Tr. Acc: ', 33.90208333333333, '%', 'Val. Loss: ', 2.3415897640069523, 'Val. Acc: ', 28.7, '%')\n",
      "('epoch: ', 17, 'Tr. loss: ', 16.979404389954066, 'Tr. Acc: ', 34.295833333333334, '%', 'Val. Loss: ', 2.3344156156165434, 'Val. Acc: ', 28.9, '%')\n",
      "('epoch: ', 18, 'Tr. loss: ', 16.858824305636865, 'Tr. Acc: ', 34.6625, '%', 'Val. Loss: ', 2.3276547166625403, 'Val. Acc: ', 28.849999999999998, '%')\n",
      "('epoch: ', 19, 'Tr. loss: ', 16.739689110142113, 'Tr. Acc: ', 35.0875, '%', 'Val. Loss: ', 2.3214284079174763, 'Val. Acc: ', 29.15, '%')\n",
      "('epoch: ', 20, 'Tr. loss: ', 16.621992769366408, 'Tr. Acc: ', 35.33541666666667, '%', 'Val. Loss: ', 2.3155768627079234, 'Val. Acc: ', 29.45, '%')\n",
      "('epoch: ', 21, 'Tr. loss: ', 16.505635650966273, 'Tr. Acc: ', 35.69791666666667, '%', 'Val. Loss: ', 2.3099432698467663, 'Val. Acc: ', 29.799999999999997, '%')\n",
      "('epoch: ', 22, 'Tr. loss: ', 16.390576534454407, 'Tr. Acc: ', 36.079166666666666, '%', 'Val. Loss: ', 2.30435205305291, 'Val. Acc: ', 29.799999999999997, '%')\n",
      "('epoch: ', 23, 'Tr. loss: ', 16.276808937611015, 'Tr. Acc: ', 36.422916666666666, '%', 'Val. Loss: ', 2.2988770768971856, 'Val. Acc: ', 29.799999999999997, '%')\n",
      "('epoch: ', 24, 'Tr. loss: ', 16.164241729487454, 'Tr. Acc: ', 36.78333333333333, '%', 'Val. Loss: ', 2.2936337335044237, 'Val. Acc: ', 29.7, '%')\n",
      "('epoch: ', 25, 'Tr. loss: ', 16.052838741126262, 'Tr. Acc: ', 37.09375, '%', 'Val. Loss: ', 2.2887922958326277, 'Val. Acc: ', 29.75, '%')\n",
      "('epoch: ', 26, 'Tr. loss: ', 15.942574662989239, 'Tr. Acc: ', 37.40833333333333, '%', 'Val. Loss: ', 2.284061382753952, 'Val. Acc: ', 29.75, '%')\n",
      "('epoch: ', 27, 'Tr. loss: ', 15.833431324893713, 'Tr. Acc: ', 37.72083333333333, '%', 'Val. Loss: ', 2.2793899089354754, 'Val. Acc: ', 30.0, '%')\n",
      "('epoch: ', 28, 'Tr. loss: ', 15.725403977040383, 'Tr. Acc: ', 37.97291666666667, '%', 'Val. Loss: ', 2.274915689187602, 'Val. Acc: ', 30.049999999999997, '%')\n",
      "('epoch: ', 29, 'Tr. loss: ', 15.61848694380945, 'Tr. Acc: ', 38.28333333333334, '%', 'Val. Loss: ', 2.2705611150973635, 'Val. Acc: ', 30.0, '%')\n",
      "('epoch: ', 30, 'Tr. loss: ', 15.51262153965753, 'Tr. Acc: ', 38.5375, '%', 'Val. Loss: ', 2.2664432777645334, 'Val. Acc: ', 30.099999999999998, '%')\n",
      "('epoch: ', 31, 'Tr. loss: ', 15.407772634877936, 'Tr. Acc: ', 38.75416666666667, '%', 'Val. Loss: ', 2.2624542086058304, 'Val. Acc: ', 30.2, '%')\n",
      "('epoch: ', 32, 'Tr. loss: ', 15.303942616523265, 'Tr. Acc: ', 39.00833333333333, '%', 'Val. Loss: ', 2.258749572327745, 'Val. Acc: ', 30.3, '%')\n",
      "('epoch: ', 33, 'Tr. loss: ', 15.201095874429638, 'Tr. Acc: ', 39.268750000000004, '%', 'Val. Loss: ', 2.2551052608049194, 'Val. Acc: ', 30.349999999999998, '%')\n",
      "('epoch: ', 34, 'Tr. loss: ', 15.09918209256676, 'Tr. Acc: ', 39.458333333333336, '%', 'Val. Loss: ', 2.2514761415951248, 'Val. Acc: ', 30.4, '%')\n",
      "('epoch: ', 35, 'Tr. loss: ', 14.998219697615502, 'Tr. Acc: ', 39.6625, '%', 'Val. Loss: ', 2.2480015289727087, 'Val. Acc: ', 30.599999999999998, '%')\n",
      "('epoch: ', 36, 'Tr. loss: ', 14.89819901556758, 'Tr. Acc: ', 39.90625, '%', 'Val. Loss: ', 2.24458916504887, 'Val. Acc: ', 30.599999999999998, '%')\n",
      "('epoch: ', 37, 'Tr. loss: ', 14.799090084631068, 'Tr. Acc: ', 40.108333333333334, '%', 'Val. Loss: ', 2.2412435439052407, 'Val. Acc: ', 30.65, '%')\n",
      "('epoch: ', 38, 'Tr. loss: ', 14.70086196895044, 'Tr. Acc: ', 40.30833333333334, '%', 'Val. Loss: ', 2.2379642806783493, 'Val. Acc: ', 30.65, '%')\n",
      "('epoch: ', 39, 'Tr. loss: ', 14.603485765217963, 'Tr. Acc: ', 40.53958333333333, '%', 'Val. Loss: ', 2.234830857337468, 'Val. Acc: ', 30.8, '%')\n",
      "('epoch: ', 40, 'Tr. loss: ', 14.507011642210134, 'Tr. Acc: ', 40.7625, '%', 'Val. Loss: ', 2.2318718283861263, 'Val. Acc: ', 30.8, '%')\n",
      "('epoch: ', 41, 'Tr. loss: ', 14.41139388999891, 'Tr. Acc: ', 40.922916666666666, '%', 'Val. Loss: ', 2.228986362202176, 'Val. Acc: ', 30.85, '%')\n",
      "('epoch: ', 42, 'Tr. loss: ', 14.316637444008329, 'Tr. Acc: ', 41.03333333333333, '%', 'Val. Loss: ', 2.226119466977866, 'Val. Acc: ', 30.95, '%')\n",
      "('epoch: ', 43, 'Tr. loss: ', 14.222710557901689, 'Tr. Acc: ', 41.22291666666666, '%', 'Val. Loss: ', 2.2233445385003416, 'Val. Acc: ', 31.0, '%')\n",
      "('epoch: ', 44, 'Tr. loss: ', 14.12960521197407, 'Tr. Acc: ', 41.35208333333333, '%', 'Val. Loss: ', 2.220588738481956, 'Val. Acc: ', 31.3, '%')\n",
      "('epoch: ', 45, 'Tr. loss: ', 14.037286076840049, 'Tr. Acc: ', 41.50416666666666, '%', 'Val. Loss: ', 2.217808294360041, 'Val. Acc: ', 31.3, '%')\n",
      "('epoch: ', 46, 'Tr. loss: ', 13.945759036613925, 'Tr. Acc: ', 41.668749999999996, '%', 'Val. Loss: ', 2.2151201594314216, 'Val. Acc: ', 31.4, '%')\n",
      "('epoch: ', 47, 'Tr. loss: ', 13.85501477837549, 'Tr. Acc: ', 41.81041666666667, '%', 'Val. Loss: ', 2.2124698292818064, 'Val. Acc: ', 31.5, '%')\n",
      "('epoch: ', 48, 'Tr. loss: ', 13.765038020555105, 'Tr. Acc: ', 41.97708333333333, '%', 'Val. Loss: ', 2.2099267434979675, 'Val. Acc: ', 31.65, '%')\n",
      "('epoch: ', 49, 'Tr. loss: ', 13.675834929987595, 'Tr. Acc: ', 42.14791666666667, '%', 'Val. Loss: ', 2.2073241074034042, 'Val. Acc: ', 31.35, '%')\n",
      "('epoch: ', 50, 'Tr. loss: ', 13.587369518814162, 'Tr. Acc: ', 42.30416666666667, '%', 'Val. Loss: ', 2.2048209899945634, 'Val. Acc: ', 31.45, '%')\n",
      "('epoch: ', 51, 'Tr. loss: ', 13.49962951503897, 'Tr. Acc: ', 42.50625, '%', 'Val. Loss: ', 2.2023397116271095, 'Val. Acc: ', 31.4, '%')\n",
      "('epoch: ', 52, 'Tr. loss: ', 13.41262882751421, 'Tr. Acc: ', 42.65833333333333, '%', 'Val. Loss: ', 2.200099665575855, 'Val. Acc: ', 31.65, '%')\n",
      "('epoch: ', 53, 'Tr. loss: ', 13.326362738666903, 'Tr. Acc: ', 42.86875, '%', 'Val. Loss: ', 2.197841093010652, 'Val. Acc: ', 31.75, '%')\n",
      "('epoch: ', 54, 'Tr. loss: ', 13.240803017058484, 'Tr. Acc: ', 43.012499999999996, '%', 'Val. Loss: ', 2.195521450586075, 'Val. Acc: ', 31.75, '%')\n",
      "('epoch: ', 55, 'Tr. loss: ', 13.155934591159186, 'Tr. Acc: ', 43.1625, '%', 'Val. Loss: ', 2.193228682603256, 'Val. Acc: ', 31.8, '%')\n",
      "('epoch: ', 56, 'Tr. loss: ', 13.071771622335797, 'Tr. Acc: ', 43.32083333333333, '%', 'Val. Loss: ', 2.1909762030304263, 'Val. Acc: ', 31.85, '%')\n",
      "('epoch: ', 57, 'Tr. loss: ', 12.988322956969746, 'Tr. Acc: ', 43.45, '%', 'Val. Loss: ', 2.188875498447193, 'Val. Acc: ', 31.900000000000002, '%')\n",
      "('epoch: ', 58, 'Tr. loss: ', 12.905543318771711, 'Tr. Acc: ', 43.577083333333334, '%', 'Val. Loss: ', 2.1868362094192997, 'Val. Acc: ', 32.05, '%')\n",
      "('epoch: ', 59, 'Tr. loss: ', 12.82341860418433, 'Tr. Acc: ', 43.735416666666666, '%', 'Val. Loss: ', 2.1848041266593246, 'Val. Acc: ', 32.15, '%')\n",
      "('epoch: ', 60, 'Tr. loss: ', 12.74198746973797, 'Tr. Acc: ', 43.84583333333333, '%', 'Val. Loss: ', 2.182772995228244, 'Val. Acc: ', 32.4, '%')\n",
      "('epoch: ', 61, 'Tr. loss: ', 12.661182729602258, 'Tr. Acc: ', 43.983333333333334, '%', 'Val. Loss: ', 2.180854947740005, 'Val. Acc: ', 32.45, '%')\n",
      "('epoch: ', 62, 'Tr. loss: ', 12.581049527788748, 'Tr. Acc: ', 44.077083333333334, '%', 'Val. Loss: ', 2.1789629467665947, 'Val. Acc: ', 32.65, '%')\n",
      "('epoch: ', 63, 'Tr. loss: ', 12.501552110220906, 'Tr. Acc: ', 44.197916666666664, '%', 'Val. Loss: ', 2.1771194248937507, 'Val. Acc: ', 32.65, '%')\n",
      "('epoch: ', 64, 'Tr. loss: ', 12.42270198501538, 'Tr. Acc: ', 44.333333333333336, '%', 'Val. Loss: ', 2.1753106162301674, 'Val. Acc: ', 32.7, '%')\n",
      "('epoch: ', 65, 'Tr. loss: ', 12.344504742410649, 'Tr. Acc: ', 44.479166666666664, '%', 'Val. Loss: ', 2.1734987532621806, 'Val. Acc: ', 32.800000000000004, '%')\n",
      "('epoch: ', 66, 'Tr. loss: ', 12.266950457116042, 'Tr. Acc: ', 44.61875, '%', 'Val. Loss: ', 2.1717088618017035, 'Val. Acc: ', 33.0, '%')\n",
      "('epoch: ', 67, 'Tr. loss: ', 12.189999223082943, 'Tr. Acc: ', 44.78125, '%', 'Val. Loss: ', 2.1698612514575717, 'Val. Acc: ', 33.15, '%')\n",
      "('epoch: ', 68, 'Tr. loss: ', 12.113677989056749, 'Tr. Acc: ', 44.864583333333336, '%', 'Val. Loss: ', 2.1681416161708342, 'Val. Acc: ', 33.15, '%')\n",
      "('epoch: ', 69, 'Tr. loss: ', 12.037951170927801, 'Tr. Acc: ', 44.95, '%', 'Val. Loss: ', 2.1663707933389644, 'Val. Acc: ', 33.300000000000004, '%')\n",
      "('epoch: ', 70, 'Tr. loss: ', 11.962826621783087, 'Tr. Acc: ', 45.05208333333333, '%', 'Val. Loss: ', 2.1646554244691307, 'Val. Acc: ', 33.4, '%')\n",
      "('epoch: ', 71, 'Tr. loss: ', 11.888298938413675, 'Tr. Acc: ', 45.177083333333336, '%', 'Val. Loss: ', 2.162972566165839, 'Val. Acc: ', 33.35, '%')\n",
      "('epoch: ', 72, 'Tr. loss: ', 11.814365655651107, 'Tr. Acc: ', 45.30208333333333, '%', 'Val. Loss: ', 2.161267158475339, 'Val. Acc: ', 33.25, '%')\n",
      "('epoch: ', 73, 'Tr. loss: ', 11.741023806142966, 'Tr. Acc: ', 45.40833333333333, '%', 'Val. Loss: ', 2.159595613338158, 'Val. Acc: ', 33.4, '%')\n",
      "('epoch: ', 74, 'Tr. loss: ', 11.668250317405937, 'Tr. Acc: ', 45.550000000000004, '%', 'Val. Loss: ', 2.157922475575687, 'Val. Acc: ', 33.650000000000006, '%')\n",
      "('epoch: ', 75, 'Tr. loss: ', 11.596058261196541, 'Tr. Acc: ', 45.65625, '%', 'Val. Loss: ', 2.156316602804827, 'Val. Acc: ', 33.800000000000004, '%')\n",
      "('epoch: ', 76, 'Tr. loss: ', 11.52444097702161, 'Tr. Acc: ', 45.764583333333334, '%', 'Val. Loss: ', 2.1547697148405436, 'Val. Acc: ', 33.85, '%')\n",
      "('epoch: ', 77, 'Tr. loss: ', 11.453382312274558, 'Tr. Acc: ', 45.85, '%', 'Val. Loss: ', 2.153158671473132, 'Val. Acc: ', 34.0, '%')\n",
      "('epoch: ', 78, 'Tr. loss: ', 11.38288324306818, 'Tr. Acc: ', 45.91875, '%', 'Val. Loss: ', 2.1516864372575304, 'Val. Acc: ', 34.0, '%')\n",
      "('epoch: ', 79, 'Tr. loss: ', 11.312928334614291, 'Tr. Acc: ', 46.075, '%', 'Val. Loss: ', 2.1501868595646263, 'Val. Acc: ', 34.0, '%')\n",
      "('epoch: ', 80, 'Tr. loss: ', 11.243532618806649, 'Tr. Acc: ', 46.177083333333336, '%', 'Val. Loss: ', 2.148652521565234, 'Val. Acc: ', 34.0, '%')\n",
      "('epoch: ', 81, 'Tr. loss: ', 11.17467452272679, 'Tr. Acc: ', 46.266666666666666, '%', 'Val. Loss: ', 2.1471741581102695, 'Val. Acc: ', 34.150000000000006, '%')\n",
      "('epoch: ', 82, 'Tr. loss: ', 11.106361957972974, 'Tr. Acc: ', 46.387499999999996, '%', 'Val. Loss: ', 2.1456305947510375, 'Val. Acc: ', 34.300000000000004, '%')\n",
      "('epoch: ', 83, 'Tr. loss: ', 11.038568255642891, 'Tr. Acc: ', 46.447916666666664, '%', 'Val. Loss: ', 2.1441375973228567, 'Val. Acc: ', 34.449999999999996, '%')\n",
      "('epoch: ', 84, 'Tr. loss: ', 10.971307355908872, 'Tr. Acc: ', 46.55625, '%', 'Val. Loss: ', 2.1425957431136373, 'Val. Acc: ', 34.5, '%')\n",
      "('epoch: ', 85, 'Tr. loss: ', 10.90457019018134, 'Tr. Acc: ', 46.677083333333336, '%', 'Val. Loss: ', 2.141100663972121, 'Val. Acc: ', 34.599999999999994, '%')\n",
      "('epoch: ', 86, 'Tr. loss: ', 10.838335459249583, 'Tr. Acc: ', 46.75, '%', 'Val. Loss: ', 2.1396907103656653, 'Val. Acc: ', 34.599999999999994, '%')\n",
      "('epoch: ', 87, 'Tr. loss: ', 10.772631953412764, 'Tr. Acc: ', 46.82083333333333, '%', 'Val. Loss: ', 2.1382923538009138, 'Val. Acc: ', 34.599999999999994, '%')\n",
      "('epoch: ', 88, 'Tr. loss: ', 10.707416199394382, 'Tr. Acc: ', 46.94166666666666, '%', 'Val. Loss: ', 2.1368894275058, 'Val. Acc: ', 34.599999999999994, '%')\n",
      "('epoch: ', 89, 'Tr. loss: ', 10.64270250461267, 'Tr. Acc: ', 47.05, '%', 'Val. Loss: ', 2.1354591325577092, 'Val. Acc: ', 34.599999999999994, '%')\n",
      "('epoch: ', 90, 'Tr. loss: ', 10.5784906098821, 'Tr. Acc: ', 47.1625, '%', 'Val. Loss: ', 2.1340529828070935, 'Val. Acc: ', 34.65, '%')\n",
      "('epoch: ', 91, 'Tr. loss: ', 10.514769747257265, 'Tr. Acc: ', 47.225, '%', 'Val. Loss: ', 2.1326734608535096, 'Val. Acc: ', 34.65, '%')\n",
      "('epoch: ', 92, 'Tr. loss: ', 10.451532724157554, 'Tr. Acc: ', 47.31666666666667, '%', 'Val. Loss: ', 2.131265283774713, 'Val. Acc: ', 34.65, '%')\n",
      "('epoch: ', 93, 'Tr. loss: ', 10.388789830762644, 'Tr. Acc: ', 47.41458333333333, '%', 'Val. Loss: ', 2.1299550395649542, 'Val. Acc: ', 34.75, '%')\n",
      "('epoch: ', 94, 'Tr. loss: ', 10.326511001450118, 'Tr. Acc: ', 47.49791666666667, '%', 'Val. Loss: ', 2.1285692760263584, 'Val. Acc: ', 34.9, '%')\n",
      "('epoch: ', 95, 'Tr. loss: ', 10.264703407455615, 'Tr. Acc: ', 47.57083333333333, '%', 'Val. Loss: ', 2.1272777521965502, 'Val. Acc: ', 35.099999999999994, '%')\n",
      "('epoch: ', 96, 'Tr. loss: ', 10.203366674353012, 'Tr. Acc: ', 47.668749999999996, '%', 'Val. Loss: ', 2.1260446760430995, 'Val. Acc: ', 35.0, '%')\n",
      "('epoch: ', 97, 'Tr. loss: ', 10.142508058047023, 'Tr. Acc: ', 47.75208333333333, '%', 'Val. Loss: ', 2.12480266011422, 'Val. Acc: ', 35.05, '%')\n",
      "('epoch: ', 98, 'Tr. loss: ', 10.08211586591922, 'Tr. Acc: ', 47.81666666666667, '%', 'Val. Loss: ', 2.123562760441023, 'Val. Acc: ', 35.199999999999996, '%')\n",
      "('epoch: ', 99, 'Tr. loss: ', 10.022183482825922, 'Tr. Acc: ', 47.90416666666666, '%', 'Val. Loss: ', 2.1223326782943546, 'Val. Acc: ', 35.199999999999996, '%')\n",
      "('epoch: ', 100, 'Tr. loss: ', 9.962695363824766, 'Tr. Acc: ', 48.03125, '%', 'Val. Loss: ', 2.1210709482903938, 'Val. Acc: ', 35.199999999999996, '%')\n",
      "('epoch: ', 101, 'Tr. loss: ', 9.903651150581275, 'Tr. Acc: ', 48.12083333333334, '%', 'Val. Loss: ', 2.1198995050219165, 'Val. Acc: ', 35.3, '%')\n",
      "('epoch: ', 102, 'Tr. loss: ', 9.845048329918281, 'Tr. Acc: ', 48.20625, '%', 'Val. Loss: ', 2.118724702158728, 'Val. Acc: ', 35.3, '%')\n",
      "('epoch: ', 103, 'Tr. loss: ', 9.786900869887091, 'Tr. Acc: ', 48.29375, '%', 'Val. Loss: ', 2.117521673152567, 'Val. Acc: ', 35.4, '%')\n",
      "('epoch: ', 104, 'Tr. loss: ', 9.72919001303868, 'Tr. Acc: ', 48.38333333333333, '%', 'Val. Loss: ', 2.1162034640833136, 'Val. Acc: ', 35.5, '%')\n",
      "('epoch: ', 105, 'Tr. loss: ', 9.671911719045719, 'Tr. Acc: ', 48.47708333333333, '%', 'Val. Loss: ', 2.1150034701186913, 'Val. Acc: ', 35.55, '%')\n",
      "('epoch: ', 106, 'Tr. loss: ', 9.615074086729994, 'Tr. Acc: ', 48.5625, '%', 'Val. Loss: ', 2.113760543123977, 'Val. Acc: ', 35.5, '%')\n",
      "('epoch: ', 107, 'Tr. loss: ', 9.558673800988844, 'Tr. Acc: ', 48.62708333333333, '%', 'Val. Loss: ', 2.112559864636388, 'Val. Acc: ', 35.449999999999996, '%')\n",
      "('epoch: ', 108, 'Tr. loss: ', 9.502691538998999, 'Tr. Acc: ', 48.706250000000004, '%', 'Val. Loss: ', 2.1113329969880734, 'Val. Acc: ', 35.4, '%')\n",
      "('epoch: ', 109, 'Tr. loss: ', 9.447136734901738, 'Tr. Acc: ', 48.802083333333336, '%', 'Val. Loss: ', 2.110124694846272, 'Val. Acc: ', 35.449999999999996, '%')\n",
      "('epoch: ', 110, 'Tr. loss: ', 9.391995781160047, 'Tr. Acc: ', 48.885416666666664, '%', 'Val. Loss: ', 2.1089734241742977, 'Val. Acc: ', 35.4, '%')\n",
      "('epoch: ', 111, 'Tr. loss: ', 9.337260231894083, 'Tr. Acc: ', 48.97083333333334, '%', 'Val. Loss: ', 2.107800000179311, 'Val. Acc: ', 35.4, '%')\n",
      "('epoch: ', 112, 'Tr. loss: ', 9.282933473788214, 'Tr. Acc: ', 49.06041666666667, '%', 'Val. Loss: ', 2.106667824725495, 'Val. Acc: ', 35.4, '%')\n",
      "('epoch: ', 113, 'Tr. loss: ', 9.22901648560105, 'Tr. Acc: ', 49.12708333333333, '%', 'Val. Loss: ', 2.105605302496803, 'Val. Acc: ', 35.5, '%')\n",
      "('epoch: ', 114, 'Tr. loss: ', 9.175503219743419, 'Tr. Acc: ', 49.231249999999996, '%', 'Val. Loss: ', 2.1045141242561667, 'Val. Acc: ', 35.55, '%')\n",
      "('epoch: ', 115, 'Tr. loss: ', 9.122382283686857, 'Tr. Acc: ', 49.295833333333334, '%', 'Val. Loss: ', 2.1034367344062077, 'Val. Acc: ', 35.65, '%')\n",
      "('epoch: ', 116, 'Tr. loss: ', 9.06967709769363, 'Tr. Acc: ', 49.364583333333336, '%', 'Val. Loss: ', 2.102351875546031, 'Val. Acc: ', 35.65, '%')\n",
      "('epoch: ', 117, 'Tr. loss: ', 9.017356466267021, 'Tr. Acc: ', 49.45, '%', 'Val. Loss: ', 2.101214830529899, 'Val. Acc: ', 35.65, '%')\n",
      "('epoch: ', 118, 'Tr. loss: ', 8.965432674791746, 'Tr. Acc: ', 49.572916666666664, '%', 'Val. Loss: ', 2.1001936564905663, 'Val. Acc: ', 35.65, '%')\n",
      "('epoch: ', 119, 'Tr. loss: ', 8.9138916697644, 'Tr. Acc: ', 49.64791666666667, '%', 'Val. Loss: ', 2.09922187510406, 'Val. Acc: ', 35.699999999999996, '%')\n",
      "('epoch: ', 120, 'Tr. loss: ', 8.862731608897718, 'Tr. Acc: ', 49.75, '%', 'Val. Loss: ', 2.098184237393327, 'Val. Acc: ', 35.65, '%')\n",
      "('epoch: ', 121, 'Tr. loss: ', 8.811949606287333, 'Tr. Acc: ', 49.84791666666666, '%', 'Val. Loss: ', 2.0971635496675143, 'Val. Acc: ', 35.699999999999996, '%')\n",
      "('epoch: ', 122, 'Tr. loss: ', 8.761546778263549, 'Tr. Acc: ', 49.920833333333334, '%', 'Val. Loss: ', 2.0962293112361694, 'Val. Acc: ', 35.8, '%')\n",
      "('epoch: ', 123, 'Tr. loss: ', 8.711525970547521, 'Tr. Acc: ', 50.016666666666666, '%', 'Val. Loss: ', 2.095271336035979, 'Val. Acc: ', 35.75, '%')\n",
      "('epoch: ', 124, 'Tr. loss: ', 8.661881826041316, 'Tr. Acc: ', 50.085416666666674, '%', 'Val. Loss: ', 2.0942981502455607, 'Val. Acc: ', 35.699999999999996, '%')\n",
      "('epoch: ', 125, 'Tr. loss: ', 8.612606893641898, 'Tr. Acc: ', 50.15833333333334, '%', 'Val. Loss: ', 2.0933505830796366, 'Val. Acc: ', 35.75, '%')\n",
      "('epoch: ', 126, 'Tr. loss: ', 8.563690322577722, 'Tr. Acc: ', 50.25416666666666, '%', 'Val. Loss: ', 2.09243979688116, 'Val. Acc: ', 35.85, '%')\n",
      "('epoch: ', 127, 'Tr. loss: ', 8.51514939485808, 'Tr. Acc: ', 50.33333333333333, '%', 'Val. Loss: ', 2.0914653823899645, 'Val. Acc: ', 36.1, '%')\n",
      "('epoch: ', 128, 'Tr. loss: ', 8.466951902686608, 'Tr. Acc: ', 50.41041666666667, '%', 'Val. Loss: ', 2.090558335823689, 'Val. Acc: ', 36.25, '%')\n",
      "('epoch: ', 129, 'Tr. loss: ', 8.419115604789953, 'Tr. Acc: ', 50.48125, '%', 'Val. Loss: ', 2.089598320815148, 'Val. Acc: ', 36.25, '%')\n",
      "('epoch: ', 130, 'Tr. loss: ', 8.371634999725856, 'Tr. Acc: ', 50.556250000000006, '%', 'Val. Loss: ', 2.0887077580521147, 'Val. Acc: ', 36.25, '%')\n",
      "('epoch: ', 131, 'Tr. loss: ', 8.324507484463242, 'Tr. Acc: ', 50.65416666666667, '%', 'Val. Loss: ', 2.0878410088716097, 'Val. Acc: ', 36.3, '%')\n",
      "('epoch: ', 132, 'Tr. loss: ', 8.277721441779855, 'Tr. Acc: ', 50.7625, '%', 'Val. Loss: ', 2.0870152975112655, 'Val. Acc: ', 36.3, '%')\n",
      "('epoch: ', 133, 'Tr. loss: ', 8.231279410912723, 'Tr. Acc: ', 50.854166666666664, '%', 'Val. Loss: ', 2.0861478689520343, 'Val. Acc: ', 36.35, '%')\n",
      "('epoch: ', 134, 'Tr. loss: ', 8.185179557162998, 'Tr. Acc: ', 50.958333333333336, '%', 'Val. Loss: ', 2.0852472053313025, 'Val. Acc: ', 36.35, '%')\n",
      "('epoch: ', 135, 'Tr. loss: ', 8.13941887226796, 'Tr. Acc: ', 51.05208333333333, '%', 'Val. Loss: ', 2.0844132220282665, 'Val. Acc: ', 36.35, '%')\n",
      "('epoch: ', 136, 'Tr. loss: ', 8.093989231087537, 'Tr. Acc: ', 51.12291666666666, '%', 'Val. Loss: ', 2.083535805016214, 'Val. Acc: ', 36.35, '%')\n",
      "('epoch: ', 137, 'Tr. loss: ', 8.048893323532482, 'Tr. Acc: ', 51.21666666666667, '%', 'Val. Loss: ', 2.0827087230696852, 'Val. Acc: ', 36.449999999999996, '%')\n",
      "('epoch: ', 138, 'Tr. loss: ', 8.004126400654547, 'Tr. Acc: ', 51.297916666666666, '%', 'Val. Loss: ', 2.0818526550207364, 'Val. Acc: ', 36.65, '%')\n",
      "('epoch: ', 139, 'Tr. loss: ', 7.9596815871218585, 'Tr. Acc: ', 51.38333333333334, '%', 'Val. Loss: ', 2.081023233928637, 'Val. Acc: ', 36.6, '%')\n",
      "('epoch: ', 140, 'Tr. loss: ', 7.91555762655969, 'Tr. Acc: ', 51.46875, '%', 'Val. Loss: ', 2.080209690760345, 'Val. Acc: ', 36.65, '%')\n",
      "('epoch: ', 141, 'Tr. loss: ', 7.871763242598254, 'Tr. Acc: ', 51.54791666666667, '%', 'Val. Loss: ', 2.0794420115194887, 'Val. Acc: ', 36.6, '%')\n",
      "('epoch: ', 142, 'Tr. loss: ', 7.828283687945415, 'Tr. Acc: ', 51.61041666666667, '%', 'Val. Loss: ', 2.0787769457456107, 'Val. Acc: ', 36.6, '%')\n",
      "('epoch: ', 143, 'Tr. loss: ', 7.78512158419625, 'Tr. Acc: ', 51.71875, '%', 'Val. Loss: ', 2.078043389466569, 'Val. Acc: ', 36.75, '%')\n",
      "('epoch: ', 144, 'Tr. loss: ', 7.742277263163861, 'Tr. Acc: ', 51.802083333333336, '%', 'Val. Loss: ', 2.077267380072288, 'Val. Acc: ', 36.85, '%')\n",
      "('epoch: ', 145, 'Tr. loss: ', 7.69975534432139, 'Tr. Acc: ', 51.889583333333334, '%', 'Val. Loss: ', 2.076496825923271, 'Val. Acc: ', 36.85, '%')\n",
      "('epoch: ', 146, 'Tr. loss: ', 7.657532154041764, 'Tr. Acc: ', 51.9375, '%', 'Val. Loss: ', 2.075763636462115, 'Val. Acc: ', 36.95, '%')\n",
      "('epoch: ', 147, 'Tr. loss: ', 7.615605314639994, 'Tr. Acc: ', 52.025, '%', 'Val. Loss: ', 2.0749410933598114, 'Val. Acc: ', 36.95, '%')\n",
      "('epoch: ', 148, 'Tr. loss: ', 7.573987914092731, 'Tr. Acc: ', 52.11666666666667, '%', 'Val. Loss: ', 2.0742095623634267, 'Val. Acc: ', 36.95, '%')\n",
      "('epoch: ', 149, 'Tr. loss: ', 7.532675048310735, 'Tr. Acc: ', 52.20625, '%', 'Val. Loss: ', 2.0735413159077556, 'Val. Acc: ', 36.95, '%')\n",
      "('epoch: ', 150, 'Tr. loss: ', 7.491666012544228, 'Tr. Acc: ', 52.287499999999994, '%', 'Val. Loss: ', 2.072765168453935, 'Val. Acc: ', 36.95, '%')\n",
      "('epoch: ', 151, 'Tr. loss: ', 7.450943194530372, 'Tr. Acc: ', 52.38333333333334, '%', 'Val. Loss: ', 2.0719881975592678, 'Val. Acc: ', 36.9, '%')\n",
      "('epoch: ', 152, 'Tr. loss: ', 7.410518930996178, 'Tr. Acc: ', 52.46666666666666, '%', 'Val. Loss: ', 2.071278882198194, 'Val. Acc: ', 36.9, '%')\n",
      "('epoch: ', 153, 'Tr. loss: ', 7.370389271807362, 'Tr. Acc: ', 52.56458333333334, '%', 'Val. Loss: ', 2.0705655306150574, 'Val. Acc: ', 36.9, '%')\n",
      "('epoch: ', 154, 'Tr. loss: ', 7.330553030069808, 'Tr. Acc: ', 52.641666666666666, '%', 'Val. Loss: ', 2.0698967019719823, 'Val. Acc: ', 36.95, '%')\n",
      "('epoch: ', 155, 'Tr. loss: ', 7.29098964805249, 'Tr. Acc: ', 52.74166666666667, '%', 'Val. Loss: ', 2.0693027499556065, 'Val. Acc: ', 37.0, '%')\n",
      "('epoch: ', 156, 'Tr. loss: ', 7.251706859632762, 'Tr. Acc: ', 52.84791666666667, '%', 'Val. Loss: ', 2.0686097704882966, 'Val. Acc: ', 37.2, '%')\n",
      "('epoch: ', 157, 'Tr. loss: ', 7.212704054362397, 'Tr. Acc: ', 52.925, '%', 'Val. Loss: ', 2.067968739635382, 'Val. Acc: ', 37.2, '%')\n",
      "('epoch: ', 158, 'Tr. loss: ', 7.173972780324662, 'Tr. Acc: ', 52.97291666666667, '%', 'Val. Loss: ', 2.067352584709288, 'Val. Acc: ', 37.2, '%')\n",
      "('epoch: ', 159, 'Tr. loss: ', 7.135534907687989, 'Tr. Acc: ', 53.07708333333333, '%', 'Val. Loss: ', 2.066652392646919, 'Val. Acc: ', 37.3, '%')\n",
      "('epoch: ', 160, 'Tr. loss: ', 7.097366489474036, 'Tr. Acc: ', 53.15625000000001, '%', 'Val. Loss: ', 2.0659905504858815, 'Val. Acc: ', 37.25, '%')\n",
      "('epoch: ', 161, 'Tr. loss: ', 7.059474641170634, 'Tr. Acc: ', 53.270833333333336, '%', 'Val. Loss: ', 2.0653740607076974, 'Val. Acc: ', 37.2, '%')\n",
      "('epoch: ', 162, 'Tr. loss: ', 7.02185980739436, 'Tr. Acc: ', 53.34166666666667, '%', 'Val. Loss: ', 2.064707343551451, 'Val. Acc: ', 37.3, '%')\n",
      "('epoch: ', 163, 'Tr. loss: ', 6.984514977487737, 'Tr. Acc: ', 53.422916666666666, '%', 'Val. Loss: ', 2.0640759611912656, 'Val. Acc: ', 37.35, '%')\n",
      "('epoch: ', 164, 'Tr. loss: ', 6.947429808870981, 'Tr. Acc: ', 53.49583333333333, '%', 'Val. Loss: ', 2.063445296709513, 'Val. Acc: ', 37.5, '%')\n",
      "('epoch: ', 165, 'Tr. loss: ', 6.910620880974208, 'Tr. Acc: ', 53.56666666666666, '%', 'Val. Loss: ', 2.062802220539491, 'Val. Acc: ', 37.55, '%')\n",
      "('epoch: ', 166, 'Tr. loss: ', 6.8740720708061875, 'Tr. Acc: ', 53.66041666666666, '%', 'Val. Loss: ', 2.062189660168537, 'Val. Acc: ', 37.55, '%')\n",
      "('epoch: ', 167, 'Tr. loss: ', 6.837782990757846, 'Tr. Acc: ', 53.733333333333334, '%', 'Val. Loss: ', 2.061646381750489, 'Val. Acc: ', 37.6, '%')\n",
      "('epoch: ', 168, 'Tr. loss: ', 6.801757651593121, 'Tr. Acc: ', 53.820833333333326, '%', 'Val. Loss: ', 2.0610632726765505, 'Val. Acc: ', 37.6, '%')\n",
      "('epoch: ', 169, 'Tr. loss: ', 6.7659893258768715, 'Tr. Acc: ', 53.90416666666666, '%', 'Val. Loss: ', 2.060478568892785, 'Val. Acc: ', 37.65, '%')\n",
      "('epoch: ', 170, 'Tr. loss: ', 6.7304723664085095, 'Tr. Acc: ', 53.98125, '%', 'Val. Loss: ', 2.0599619484746854, 'Val. Acc: ', 37.6, '%')\n",
      "('epoch: ', 171, 'Tr. loss: ', 6.69521492232354, 'Tr. Acc: ', 54.07291666666667, '%', 'Val. Loss: ', 2.059444794503144, 'Val. Acc: ', 37.65, '%')\n",
      "('epoch: ', 172, 'Tr. loss: ', 6.660208913859341, 'Tr. Acc: ', 54.1625, '%', 'Val. Loss: ', 2.058956227695578, 'Val. Acc: ', 37.7, '%')\n",
      "('epoch: ', 173, 'Tr. loss: ', 6.625458658136628, 'Tr. Acc: ', 54.210416666666674, '%', 'Val. Loss: ', 2.0584086964359374, 'Val. Acc: ', 37.75, '%')\n",
      "('epoch: ', 174, 'Tr. loss: ', 6.5909472162295195, 'Tr. Acc: ', 54.26875, '%', 'Val. Loss: ', 2.0578930166618794, 'Val. Acc: ', 37.75, '%')\n",
      "('epoch: ', 175, 'Tr. loss: ', 6.556678329667006, 'Tr. Acc: ', 54.35, '%', 'Val. Loss: ', 2.057470452486172, 'Val. Acc: ', 37.8, '%')\n",
      "('epoch: ', 176, 'Tr. loss: ', 6.522667028581452, 'Tr. Acc: ', 54.43958333333333, '%', 'Val. Loss: ', 2.0569553674826895, 'Val. Acc: ', 37.8, '%')\n",
      "('epoch: ', 177, 'Tr. loss: ', 6.48888209431352, 'Tr. Acc: ', 54.57291666666667, '%', 'Val. Loss: ', 2.056467459807143, 'Val. Acc: ', 37.75, '%')\n",
      "('epoch: ', 178, 'Tr. loss: ', 6.455339410690067, 'Tr. Acc: ', 54.63958333333333, '%', 'Val. Loss: ', 2.0560713240618784, 'Val. Acc: ', 37.85, '%')\n",
      "('epoch: ', 179, 'Tr. loss: ', 6.422037217018651, 'Tr. Acc: ', 54.74791666666666, '%', 'Val. Loss: ', 2.05566212942254, 'Val. Acc: ', 37.85, '%')\n",
      "('epoch: ', 180, 'Tr. loss: ', 6.388968268777691, 'Tr. Acc: ', 54.82083333333333, '%', 'Val. Loss: ', 2.055286778169762, 'Val. Acc: ', 37.9, '%')\n",
      "('epoch: ', 181, 'Tr. loss: ', 6.356134593399182, 'Tr. Acc: ', 54.91666666666667, '%', 'Val. Loss: ', 2.0548760808098385, 'Val. Acc: ', 37.85, '%')\n",
      "('epoch: ', 182, 'Tr. loss: ', 6.323546518163922, 'Tr. Acc: ', 54.985416666666666, '%', 'Val. Loss: ', 2.0544446010349566, 'Val. Acc: ', 37.85, '%')\n",
      "('epoch: ', 183, 'Tr. loss: ', 6.291180231643572, 'Tr. Acc: ', 55.06875, '%', 'Val. Loss: ', 2.0540340006668876, 'Val. Acc: ', 37.85, '%')\n",
      "('epoch: ', 184, 'Tr. loss: ', 6.25905281285225, 'Tr. Acc: ', 55.18958333333334, '%', 'Val. Loss: ', 2.0535721010106536, 'Val. Acc: ', 37.95, '%')\n",
      "('epoch: ', 185, 'Tr. loss: ', 6.227147576782067, 'Tr. Acc: ', 55.293749999999996, '%', 'Val. Loss: ', 2.053163932188663, 'Val. Acc: ', 38.1, '%')\n",
      "('epoch: ', 186, 'Tr. loss: ', 6.1954751683690095, 'Tr. Acc: ', 55.39375, '%', 'Val. Loss: ', 2.0528094770624, 'Val. Acc: ', 38.1, '%')\n",
      "('epoch: ', 187, 'Tr. loss: ', 6.164020433851224, 'Tr. Acc: ', 55.46875, '%', 'Val. Loss: ', 2.052459067077788, 'Val. Acc: ', 38.05, '%')\n",
      "('epoch: ', 188, 'Tr. loss: ', 6.132794462726607, 'Tr. Acc: ', 55.5625, '%', 'Val. Loss: ', 2.052115301216083, 'Val. Acc: ', 38.15, '%')\n",
      "('epoch: ', 189, 'Tr. loss: ', 6.101784595144227, 'Tr. Acc: ', 55.677083333333336, '%', 'Val. Loss: ', 2.0517784115118665, 'Val. Acc: ', 38.15, '%')\n",
      "('epoch: ', 190, 'Tr. loss: ', 6.07100013520158, 'Tr. Acc: ', 55.75833333333333, '%', 'Val. Loss: ', 2.051458513772043, 'Val. Acc: ', 38.2, '%')\n",
      "('epoch: ', 191, 'Tr. loss: ', 6.040433779692088, 'Tr. Acc: ', 55.82708333333334, '%', 'Val. Loss: ', 2.05113890346016, 'Val. Acc: ', 38.25, '%')\n",
      "('epoch: ', 192, 'Tr. loss: ', 6.010074631723008, 'Tr. Acc: ', 55.92916666666666, '%', 'Val. Loss: ', 2.050781420012904, 'Val. Acc: ', 38.3, '%')\n",
      "('epoch: ', 193, 'Tr. loss: ', 5.979938060121035, 'Tr. Acc: ', 56.041666666666664, '%', 'Val. Loss: ', 2.05050831764496, 'Val. Acc: ', 38.35, '%')\n",
      "('epoch: ', 194, 'Tr. loss: ', 5.950014960715515, 'Tr. Acc: ', 56.12708333333334, '%', 'Val. Loss: ', 2.0501751244742543, 'Val. Acc: ', 38.45, '%')\n",
      "('epoch: ', 195, 'Tr. loss: ', 5.920308537954002, 'Tr. Acc: ', 56.22083333333333, '%', 'Val. Loss: ', 2.049897103078713, 'Val. Acc: ', 38.45, '%')\n",
      "('epoch: ', 196, 'Tr. loss: ', 5.890802255086571, 'Tr. Acc: ', 56.318749999999994, '%', 'Val. Loss: ', 2.049613395370806, 'Val. Acc: ', 38.35, '%')\n",
      "('epoch: ', 197, 'Tr. loss: ', 5.861509024821821, 'Tr. Acc: ', 56.40416666666667, '%', 'Val. Loss: ', 2.0493298487938474, 'Val. Acc: ', 38.35, '%')\n",
      "('epoch: ', 198, 'Tr. loss: ', 5.832414257890395, 'Tr. Acc: ', 56.50208333333333, '%', 'Val. Loss: ', 2.0490598913771856, 'Val. Acc: ', 38.45, '%')\n",
      "('epoch: ', 199, 'Tr. loss: ', 5.803517540705011, 'Tr. Acc: ', 56.58541666666667, '%', 'Val. Loss: ', 2.048739426487541, 'Val. Acc: ', 38.5, '%')\n",
      "('epoch: ', 200, 'Tr. loss: ', 5.774827455913752, 'Tr. Acc: ', 56.68541666666667, '%', 'Val. Loss: ', 2.0484484680990374, 'Val. Acc: ', 38.45, '%')\n",
      "('epoch: ', 201, 'Tr. loss: ', 5.746341530678417, 'Tr. Acc: ', 56.77291666666666, '%', 'Val. Loss: ', 2.0482460657371964, 'Val. Acc: ', 38.45, '%')\n",
      "('epoch: ', 202, 'Tr. loss: ', 5.718049796568649, 'Tr. Acc: ', 56.87708333333333, '%', 'Val. Loss: ', 2.0479527189197984, 'Val. Acc: ', 38.550000000000004, '%')\n",
      "('epoch: ', 203, 'Tr. loss: ', 5.689953867046986, 'Tr. Acc: ', 56.97291666666666, '%', 'Val. Loss: ', 2.0476460141769013, 'Val. Acc: ', 38.45, '%')\n",
      "('epoch: ', 204, 'Tr. loss: ', 5.662066809361738, 'Tr. Acc: ', 57.047916666666666, '%', 'Val. Loss: ', 2.0473372484481502, 'Val. Acc: ', 38.5, '%')\n",
      "('epoch: ', 205, 'Tr. loss: ', 5.634370601422612, 'Tr. Acc: ', 57.16250000000001, '%', 'Val. Loss: ', 2.047100839173135, 'Val. Acc: ', 38.65, '%')\n",
      "('epoch: ', 206, 'Tr. loss: ', 5.606870359031144, 'Tr. Acc: ', 57.29791666666667, '%', 'Val. Loss: ', 2.0468234667299177, 'Val. Acc: ', 38.7, '%')\n",
      "('epoch: ', 207, 'Tr. loss: ', 5.579560528556789, 'Tr. Acc: ', 57.395833333333336, '%', 'Val. Loss: ', 2.0465632178798336, 'Val. Acc: ', 38.65, '%')\n",
      "('epoch: ', 208, 'Tr. loss: ', 5.552439181142447, 'Tr. Acc: ', 57.489583333333336, '%', 'Val. Loss: ', 2.0462982284373394, 'Val. Acc: ', 38.65, '%')\n",
      "('epoch: ', 209, 'Tr. loss: ', 5.525511421780631, 'Tr. Acc: ', 57.574999999999996, '%', 'Val. Loss: ', 2.0461006859840647, 'Val. Acc: ', 38.7, '%')\n",
      "('epoch: ', 210, 'Tr. loss: ', 5.498769392630759, 'Tr. Acc: ', 57.64791666666667, '%', 'Val. Loss: ', 2.0458882084071086, 'Val. Acc: ', 38.6, '%')\n",
      "('epoch: ', 211, 'Tr. loss: ', 5.472211198695264, 'Tr. Acc: ', 57.76458333333333, '%', 'Val. Loss: ', 2.045713680008177, 'Val. Acc: ', 38.550000000000004, '%')\n",
      "('epoch: ', 212, 'Tr. loss: ', 5.445838446072516, 'Tr. Acc: ', 57.83541666666666, '%', 'Val. Loss: ', 2.045530573541966, 'Val. Acc: ', 38.6, '%')\n",
      "('epoch: ', 213, 'Tr. loss: ', 5.419645325090146, 'Tr. Acc: ', 57.91041666666666, '%', 'Val. Loss: ', 2.0453321808193743, 'Val. Acc: ', 38.6, '%')\n",
      "('epoch: ', 214, 'Tr. loss: ', 5.393630198186539, 'Tr. Acc: ', 57.989583333333336, '%', 'Val. Loss: ', 2.0451690838708823, 'Val. Acc: ', 38.65, '%')\n",
      "('epoch: ', 215, 'Tr. loss: ', 5.367795918878524, 'Tr. Acc: ', 58.087500000000006, '%', 'Val. Loss: ', 2.0450040297323544, 'Val. Acc: ', 38.7, '%')\n",
      "('epoch: ', 216, 'Tr. loss: ', 5.342139583367743, 'Tr. Acc: ', 58.1875, '%', 'Val. Loss: ', 2.0448811569116967, 'Val. Acc: ', 38.7, '%')\n",
      "('epoch: ', 217, 'Tr. loss: ', 5.316664737904851, 'Tr. Acc: ', 58.2875, '%', 'Val. Loss: ', 2.044702139895982, 'Val. Acc: ', 38.75, '%')\n",
      "('epoch: ', 218, 'Tr. loss: ', 5.291359757124387, 'Tr. Acc: ', 58.37708333333333, '%', 'Val. Loss: ', 2.044643271669397, 'Val. Acc: ', 38.65, '%')\n",
      "('epoch: ', 219, 'Tr. loss: ', 5.266233146305229, 'Tr. Acc: ', 58.481249999999996, '%', 'Val. Loss: ', 2.0445593694683706, 'Val. Acc: ', 38.800000000000004, '%')\n",
      "('epoch: ', 220, 'Tr. loss: ', 5.241264312863069, 'Tr. Acc: ', 58.59583333333334, '%', 'Val. Loss: ', 2.0445457198098214, 'Val. Acc: ', 38.85, '%')\n",
      "('epoch: ', 221, 'Tr. loss: ', 5.216481419715022, 'Tr. Acc: ', 58.71666666666666, '%', 'Val. Loss: ', 2.0444698577889553, 'Val. Acc: ', 38.800000000000004, '%')\n",
      "('epoch: ', 222, 'Tr. loss: ', 5.19186569389443, 'Tr. Acc: ', 58.825, '%', 'Val. Loss: ', 2.0444572715407947, 'Val. Acc: ', 38.800000000000004, '%')\n",
      "('epoch: ', 223, 'Tr. loss: ', 5.167416209681881, 'Tr. Acc: ', 58.9375, '%', 'Val. Loss: ', 2.0444354386140455, 'Val. Acc: ', 38.800000000000004, '%')\n",
      "('epoch: ', 224, 'Tr. loss: ', 5.1431380552664345, 'Tr. Acc: ', 59.043749999999996, '%', 'Val. Loss: ', 2.0444489555018466, 'Val. Acc: ', 38.9, '%')\n",
      "('epoch: ', 225, 'Tr. loss: ', 5.119030374901841, 'Tr. Acc: ', 59.14791666666667, '%', 'Val. Loss: ', 2.0444529012917387, 'Val. Acc: ', 38.9, '%')\n",
      "('epoch: ', 226, 'Tr. loss: ', 5.09508623791335, 'Tr. Acc: ', 59.22291666666667, '%', 'Val. Loss: ', 2.044452850391492, 'Val. Acc: ', 38.95, '%')\n",
      "('epoch: ', 227, 'Tr. loss: ', 5.071305871549447, 'Tr. Acc: ', 59.30833333333333, '%', 'Val. Loss: ', 2.0444594104345675, 'Val. Acc: ', 38.9, '%')\n",
      "('epoch: ', 228, 'Tr. loss: ', 5.047703206531089, 'Tr. Acc: ', 59.395833333333336, '%', 'Val. Loss: ', 2.0444404424262785, 'Val. Acc: ', 38.95, '%')\n",
      "('epoch: ', 229, 'Tr. loss: ', 5.024253895617862, 'Tr. Acc: ', 59.485416666666666, '%', 'Val. Loss: ', 2.0445206178747752, 'Val. Acc: ', 38.95, '%')\n",
      "('test data - loss: ', 2.0314163480466214, 'Acc: ', 38.129999999999995, '%')\n",
      "('class: ', 0, ' acc: ', 36.4)\n",
      "('class: ', 1, ' acc: ', 38.0)\n",
      "('class: ', 2, ' acc: ', 57.4)\n",
      "('class: ', 3, ' acc: ', 40.2)\n",
      "('class: ', 4, ' acc: ', 46.0)\n",
      "('class: ', 5, ' acc: ', 27.400000000000002)\n",
      "('class: ', 6, ' acc: ', 47.0)\n",
      "('class: ', 7, ' acc: ', 42.4)\n",
      "('class: ', 8, ' acc: ', 27.6)\n",
      "('class: ', 9, ' acc: ', 53.2)\n",
      "('class: ', 10, ' acc: ', 58.599999999999994)\n",
      "('class: ', 11, ' acc: ', 33.6)\n",
      "('class: ', 12, ' acc: ', 26.0)\n",
      "('class: ', 13, ' acc: ', 13.200000000000001)\n",
      "('class: ', 14, ' acc: ', 43.0)\n",
      "('class: ', 15, ' acc: ', 21.4)\n",
      "('class: ', 16, ' acc: ', 15.0)\n",
      "('class: ', 17, ' acc: ', 67.0)\n",
      "('class: ', 18, ' acc: ', 34.4)\n",
      "('class: ', 19, ' acc: ', 34.8)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1dfc0e3810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training data preprocessing\n",
    "\n",
    "Darr = Darr - np.mean(Darr, axis=0) #zero centering the data\n",
    "Darr = Darr / np.std(Darr, axis=0)  #normalizing the data\n",
    "\n",
    "#test data preprocessing\n",
    "Tarr = Tarr - np.mean(Tarr, axis=0) #zero centering the data\n",
    "Tarr = Tarr / np.std(Tarr, axis=0) #normalizing the data\n",
    "\n",
    "#the random seed used\n",
    "np.random.seed(7);\n",
    "#the architecture as follows: inputSize = 3072, outputSize = 20, Number of Hidden Layers = 4, the Hidden layer sizes = [1024,512,256,128], the input activation layer = 'relu',the hidden layer activation = 'relu', reg = 0.01, learnning Rate = 0.001, ToStop = True, Loss history cache \n",
    "nn = NeuralNetwork(Darr[0].shape[0],20,3,[1024,512,128],'relu','relu',0.01,0.001,True, 5);\n",
    "\n",
    "\n",
    "nn.train(Darr,Larr,400,128,2000); #training(inputData, Labels, Epochs, batchSize, Validation set size);\n",
    "nn.test(Tarr, TLarr); #training(inputData, Labels);\n",
    "nn.testPerClass(Tarr,TLarr,20);  #training(inputData, Labels, number of classes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
